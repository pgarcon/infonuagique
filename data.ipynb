{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2721583-dd88-49a7-9630-7166ae1a049a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0504d4-f359-4414-930b-31b5c79e3cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 09:30:15.516485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be9759-19c9-46e8-aabc-e5b68d54bd0e",
   "metadata": {},
   "source": [
    "### Récuperation des données qui sont dans le CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8f5de6-173d-4f99-8185-871c9663b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_csv('datas/datas.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720bcec-9674-4a75-9485-c6444bac276b",
   "metadata": {},
   "source": [
    "### Récupération de la date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "616872ff-2023-449b-b446-c45f154118d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = datas.columns.drop('Time')\n",
    "\n",
    "datas['Day'] = datas.Time.apply(lambda x: x.split('-')[-1])\n",
    "datas['Month'] = datas.Time.apply(lambda x: x.split('-')[1])\n",
    "datas['Year'] = datas.Time.apply(lambda x: x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f8a4e3-a21a-4028-b2bc-c4905a6811a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>BTC / Price, USD</th>\n",
       "      <th>ETH / Price, USD</th>\n",
       "      <th>XRP / Price, USD</th>\n",
       "      <th>LINK / Price, USD</th>\n",
       "      <th>XLM / Price, USD</th>\n",
       "      <th>USDT / Price, USD</th>\n",
       "      <th>USDT_ETH / Price, USD</th>\n",
       "      <th>USDT_TRX / Price, USD</th>\n",
       "      <th>DASH / Price, USD</th>\n",
       "      <th>...</th>\n",
       "      <th>WTC / Price, USD</th>\n",
       "      <th>XEM / Price, USD</th>\n",
       "      <th>XMR / Price, USD</th>\n",
       "      <th>XVG / Price, USD</th>\n",
       "      <th>XTZ / Price, USD</th>\n",
       "      <th>YFI / Price, USD</th>\n",
       "      <th>ZEC / Price, USD</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>1228.459651</td>\n",
       "      <td>17.265684</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.279388</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>12.377001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.565084</td>\n",
       "      <td>01</td>\n",
       "      <td>03</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-02</td>\n",
       "      <td>1262.931467</td>\n",
       "      <td>19.204561</td>\n",
       "      <td>0.006133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>1.006446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.546546</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>13.213293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.045525</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-03</td>\n",
       "      <td>1289.363191</td>\n",
       "      <td>19.654703</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>1.003244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.777786</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009055</td>\n",
       "      <td>14.029060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.348426</td>\n",
       "      <td>03</td>\n",
       "      <td>03</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-03-04</td>\n",
       "      <td>1268.940664</td>\n",
       "      <td>18.684225</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>1.005842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.735475</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>13.808267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.372994</td>\n",
       "      <td>04</td>\n",
       "      <td>03</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-03-05</td>\n",
       "      <td>1276.273683</td>\n",
       "      <td>19.301111</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.373144</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>15.263604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.567362</td>\n",
       "      <td>05</td>\n",
       "      <td>03</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>43614.342696</td>\n",
       "      <td>3080.565482</td>\n",
       "      <td>0.834418</td>\n",
       "      <td>17.553471</td>\n",
       "      <td>0.232654</td>\n",
       "      <td>1.000671</td>\n",
       "      <td>1.000671</td>\n",
       "      <td>1.000671</td>\n",
       "      <td>111.891064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593469</td>\n",
       "      <td>0.114365</td>\n",
       "      <td>180.655497</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>4.152122</td>\n",
       "      <td>24498.512741</td>\n",
       "      <td>127.082193</td>\n",
       "      <td>10</td>\n",
       "      <td>02</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>2022-02-11</td>\n",
       "      <td>42357.321742</td>\n",
       "      <td>2924.080468</td>\n",
       "      <td>0.759247</td>\n",
       "      <td>16.146358</td>\n",
       "      <td>0.215302</td>\n",
       "      <td>1.000517</td>\n",
       "      <td>1.000517</td>\n",
       "      <td>1.000517</td>\n",
       "      <td>103.997657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550347</td>\n",
       "      <td>0.108235</td>\n",
       "      <td>168.596810</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>4.220464</td>\n",
       "      <td>23068.136834</td>\n",
       "      <td>116.299055</td>\n",
       "      <td>11</td>\n",
       "      <td>02</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>2022-02-12</td>\n",
       "      <td>42173.459396</td>\n",
       "      <td>2910.836175</td>\n",
       "      <td>0.819350</td>\n",
       "      <td>15.943319</td>\n",
       "      <td>0.217710</td>\n",
       "      <td>1.000562</td>\n",
       "      <td>1.000562</td>\n",
       "      <td>1.000562</td>\n",
       "      <td>103.734638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552108</td>\n",
       "      <td>0.108695</td>\n",
       "      <td>173.587783</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>4.089775</td>\n",
       "      <td>23100.542446</td>\n",
       "      <td>117.484142</td>\n",
       "      <td>12</td>\n",
       "      <td>02</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>2022-02-13</td>\n",
       "      <td>42195.594745</td>\n",
       "      <td>2880.696526</td>\n",
       "      <td>0.809239</td>\n",
       "      <td>15.718898</td>\n",
       "      <td>0.211419</td>\n",
       "      <td>1.000460</td>\n",
       "      <td>1.000460</td>\n",
       "      <td>1.000460</td>\n",
       "      <td>102.548106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546389</td>\n",
       "      <td>0.110236</td>\n",
       "      <td>174.332235</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>3.871521</td>\n",
       "      <td>22934.195769</td>\n",
       "      <td>117.312547</td>\n",
       "      <td>13</td>\n",
       "      <td>02</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>42646.161859</td>\n",
       "      <td>2936.298260</td>\n",
       "      <td>0.803149</td>\n",
       "      <td>15.957323</td>\n",
       "      <td>0.211036</td>\n",
       "      <td>1.000487</td>\n",
       "      <td>1.000487</td>\n",
       "      <td>1.000487</td>\n",
       "      <td>104.046215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531333</td>\n",
       "      <td>0.109293</td>\n",
       "      <td>176.669049</td>\n",
       "      <td>0.011313</td>\n",
       "      <td>3.941494</td>\n",
       "      <td>23146.461202</td>\n",
       "      <td>120.929534</td>\n",
       "      <td>14</td>\n",
       "      <td>02</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1812 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time  BTC / Price, USD  ETH / Price, USD  XRP / Price, USD  \\\n",
       "0     2017-03-01       1228.459651         17.265684          0.005330   \n",
       "1     2017-03-02       1262.931467         19.204561          0.006133   \n",
       "2     2017-03-03       1289.363191         19.654703          0.006466   \n",
       "3     2017-03-04       1268.940664         18.684225          0.006247   \n",
       "4     2017-03-05       1276.273683         19.301111          0.006151   \n",
       "...          ...               ...               ...               ...   \n",
       "1807  2022-02-10      43614.342696       3080.565482          0.834418   \n",
       "1808  2022-02-11      42357.321742       2924.080468          0.759247   \n",
       "1809  2022-02-12      42173.459396       2910.836175          0.819350   \n",
       "1810  2022-02-13      42195.594745       2880.696526          0.809239   \n",
       "1811  2022-02-14      42646.161859       2936.298260          0.803149   \n",
       "\n",
       "      LINK / Price, USD  XLM / Price, USD  USDT / Price, USD  \\\n",
       "0                   NaN          0.001731           0.999667   \n",
       "1                   NaN          0.001799           1.006446   \n",
       "2                   NaN          0.001789           1.003244   \n",
       "3                   NaN          0.001750           1.005842   \n",
       "4                   NaN          0.001742           0.999961   \n",
       "...                 ...               ...                ...   \n",
       "1807          17.553471          0.232654           1.000671   \n",
       "1808          16.146358          0.215302           1.000517   \n",
       "1809          15.943319          0.217710           1.000562   \n",
       "1810          15.718898          0.211419           1.000460   \n",
       "1811          15.957323          0.211036           1.000487   \n",
       "\n",
       "      USDT_ETH / Price, USD  USDT_TRX / Price, USD  DASH / Price, USD  ...  \\\n",
       "0                       NaN                    NaN          43.279388  ...   \n",
       "1                       NaN                    NaN          42.546546  ...   \n",
       "2                       NaN                    NaN          47.777786  ...   \n",
       "3                       NaN                    NaN          43.735475  ...   \n",
       "4                       NaN                    NaN          42.373144  ...   \n",
       "...                     ...                    ...                ...  ...   \n",
       "1807               1.000671               1.000671         111.891064  ...   \n",
       "1808               1.000517               1.000517         103.997657  ...   \n",
       "1809               1.000562               1.000562         103.734638  ...   \n",
       "1810               1.000460               1.000460         102.548106  ...   \n",
       "1811               1.000487               1.000487         104.046215  ...   \n",
       "\n",
       "      WTC / Price, USD  XEM / Price, USD  XMR / Price, USD  XVG / Price, USD  \\\n",
       "0                  NaN          0.007874         12.377001               NaN   \n",
       "1                  NaN          0.008749         13.213293               NaN   \n",
       "2                  NaN          0.009055         14.029060               NaN   \n",
       "3                  NaN          0.009835         13.808267               NaN   \n",
       "4                  NaN          0.011450         15.263604               NaN   \n",
       "...                ...               ...               ...               ...   \n",
       "1807          0.593469          0.114365        180.655497          0.012299   \n",
       "1808          0.550347          0.108235        168.596810          0.011535   \n",
       "1809          0.552108          0.108695        173.587783          0.011503   \n",
       "1810          0.546389          0.110236        174.332235          0.011014   \n",
       "1811          0.531333          0.109293        176.669049          0.011313   \n",
       "\n",
       "      XTZ / Price, USD  YFI / Price, USD  ZEC / Price, USD  Day  Month  Year  \n",
       "0                  NaN               NaN         41.565084   01     03  2017  \n",
       "1                  NaN               NaN         40.045525   02     03  2017  \n",
       "2                  NaN               NaN         42.348426   03     03  2017  \n",
       "3                  NaN               NaN         41.372994   04     03  2017  \n",
       "4                  NaN               NaN         39.567362   05     03  2017  \n",
       "...                ...               ...               ...  ...    ...   ...  \n",
       "1807          4.152122      24498.512741        127.082193   10     02  2022  \n",
       "1808          4.220464      23068.136834        116.299055   11     02  2022  \n",
       "1809          4.089775      23100.542446        117.484142   12     02  2022  \n",
       "1810          3.871521      22934.195769        117.312547   13     02  2022  \n",
       "1811          3.941494      23146.461202        120.929534   14     02  2022  \n",
       "\n",
       "[1812 rows x 104 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2377abca-c9a9-4683-bc8e-429b4ccce292",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_crypto_df = {}\n",
    "liste_title = []\n",
    "for crypto in headers:\n",
    "    df = pd.DataFrame(datas[crypto])\n",
    "\n",
    "    title = crypto.split(' ')[0]\n",
    "    liste_title.append(title)\n",
    "    liste_crypto_df[title] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c6b9d6-83a9-4203-9d72-807501809665",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crypto_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(crypto_dataset))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Avant de passer les données à votre modèle, normalisez-les\u001b[39;00m\n\u001b[1;32m      4\u001b[0m mean \u001b[38;5;241m=\u001b[39m crypto_dataset\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crypto_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(crypto_dataset))\n",
    "\n",
    "# Avant de passer les données à votre modèle, normalisez-les\n",
    "mean = crypto_dataset.dataframe.mean()\n",
    "std = crypto_dataset.dataframe.std()\n",
    "\n",
    "crypto_dataset.dataframe = (crypto_dataset.dataframe - mean) / std\n",
    "\n",
    "train_size = int(0.8 * len(crypto_dataset))  # Utilisation de 80% des données pour l'entraînement, ajustez si nécessaire\n",
    "test_size = len(crypto_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(crypto_dataset, [train_size, test_size])\n",
    "\n",
    "# Vérifier les tailles des ensembles de données\n",
    "print(\"Taille de l'ensemble d'entraînement :\", len(train_dataset))\n",
    "print(\"Taille de l'ensemble de test :\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3319e376-29d5-4bc7-8026-4cea607d9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le DataLoader\n",
    "batch_size = 100  # Choisir la taille du lot en fonction de vos besoins\n",
    "crypto_dataloader = DataLoader(train_dataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "crypto_dataloader_test = DataLoader(test_dataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb1502e-e8f6-444d-8275-41d8014c4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoPredict(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super(CryptoPredict, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.batch_norm = nn.BatchNorm1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Usage of the model\n",
    "input_size = 1\n",
    "# Usage of the model\n",
    "hidden_size = 40\n",
    "num_stacked_layers = 1  # You may adjust this value\n",
    "\n",
    "model = CryptoPredict(input_size, hidden_size, num_stacked_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "92ca7fd4-67b2-49ab-b6eb-70c24b1f2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "cc918cce-5565-42da-aa94-d3a0b7362b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800, Loss: 2301140.2992412862\n",
      "Epoch 2/800, Loss: 2295223.7459810697\n",
      "Epoch 3/800, Loss: 2291310.5788010815\n",
      "Epoch 4/800, Loss: 2288152.1503342846\n",
      "Epoch 5/800, Loss: 2285239.8457594654\n",
      "Epoch 6/800, Loss: 2282348.7649113582\n",
      "Epoch 7/800, Loss: 2279524.7591458834\n",
      "Epoch 8/800, Loss: 2276828.0467059794\n",
      "Epoch 9/800, Loss: 2273951.452843299\n",
      "Epoch 10/800, Loss: 2271153.3064434347\n",
      "Epoch 11/800, Loss: 2268491.6288686898\n",
      "Epoch 12/800, Loss: 2265885.2790527344\n",
      "Epoch 13/800, Loss: 2263318.325063852\n",
      "Epoch 14/800, Loss: 2260782.6066331128\n",
      "Epoch 15/800, Loss: 2258273.0309213493\n",
      "Epoch 16/800, Loss: 2255786.1378079928\n",
      "Epoch 17/800, Loss: 2253319.4059213493\n",
      "Epoch 18/800, Loss: 2250870.410888672\n",
      "Epoch 19/800, Loss: 2248438.1301739034\n",
      "Epoch 20/800, Loss: 2246021.049128606\n",
      "Epoch 21/800, Loss: 2243618.1212252104\n",
      "Epoch 22/800, Loss: 2241228.4814453125\n",
      "Epoch 23/800, Loss: 2238851.3658259464\n",
      "Epoch 24/800, Loss: 2236486.149104192\n",
      "Epoch 25/800, Loss: 2234132.4513502857\n",
      "Epoch 26/800, Loss: 2231789.6564847506\n",
      "Epoch 27/800, Loss: 2229457.282273513\n",
      "Epoch 28/800, Loss: 2227135.1904484676\n",
      "Epoch 29/800, Loss: 2224822.8252093974\n",
      "Epoch 30/800, Loss: 2222520.0111600435\n",
      "Epoch 31/800, Loss: 2220226.4702477087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(\"output : \", outputs)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(\"labels : \", labels)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epoch = 800\n",
    "\n",
    "for epoch in range(num_epoch): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(crypto_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        #print(\"output : \", outputs)\n",
    "        #print(\"labels : \", labels)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    print(f'Epoch {epoch + 1}/{num_epoch}, Loss: {running_loss / len(crypto_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22719d-7c0a-444a-bc5e-48727443cd51",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52a10dd-0f6f-4b3f-8bcb-f45aea49065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = liste_crypto_df['ETH'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e87adcc8-2b3b-4ecf-9527-4554fc21ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1441, 10, 1)\n",
      "X_test shape: (180, 10, 1)\n",
      "y_train shape: (1441, 1)\n",
      "y_test shape: (180, 1)\n"
     ]
    }
   ],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = liste\n",
    "# choose a number of time steps\n",
    "n_steps = 10\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "\n",
    "# Supposons que X et y soient déjà définis à partir de la fonction split_sequence\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test, X_dev, y_test, y_dev = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Afficher les dimensions des ensembles de données\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Normaliser les données d'entrée X_train et X_test\n",
    "X_train = scaler_x.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler_x.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "X_dev = scaler_x.transform(X_dev.reshape(-1, 1)).reshape(X_dev.shape)\n",
    "\n",
    "# Normaliser les données de sortie y_train et y_test\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "y_test = scaler_y.transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "y_dev = scaler_y.transform(y_dev.reshape(-1, 1)).reshape(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49e343c8-8ef1-4e1c-923f-f1f8fa07c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test, y_test, sequence_length):\n",
    "    \n",
    "    # Définir le modèle\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(70, activation='relu', input_shape=(sequence_length, 1)))  # Couche LSTM avec return_sequences=True\n",
    "    model.add(Dense(1))  # Couche Dense finale\n",
    "    \n",
    "    # Compiler le modèle\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ffaa7e5-37b7-43d4-affa-95bdf0e6343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23/23 [==============================] - 2s 46ms/step - loss: 0.5693 - val_loss: 0.0989\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.0652 - val_loss: 0.0475\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0325 - val_loss: 0.0168\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 1s 21ms/step - loss: 0.0190 - val_loss: 0.0119\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0109 - val_loss: 0.0060\n"
     ]
    }
   ],
   "source": [
    "model_test, history = train(X_train, y_train, X_dev, y_dev, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8e7e2ab-978e-4fd7-a03a-4896cfe6045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21048146]\n",
      " [-0.19669718]\n",
      " [-0.17792214]\n",
      " [-0.12326292]\n",
      " [-0.20246965]\n",
      " [-0.16109757]\n",
      " [-0.17238114]\n",
      " [-0.14293881]\n",
      " [-0.13788614]\n",
      " [-0.15648613]]   :   [-0.160511]\n"
     ]
    }
   ],
   "source": [
    "n = 8\n",
    "\n",
    "test_prediction = X_test[n]\n",
    "value = y_test[n]\n",
    "\n",
    "print(test_prediction, \"  :  \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc54d871-dedd-4519-8fd0-b7b7926cf46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037745673\n",
      "[[899.45215]]\n"
     ]
    }
   ],
   "source": [
    "yhat = model_test.predict(test_prediction, verbose=0)\n",
    "\n",
    "average_prediction = np.mean(yhat)\n",
    "\n",
    "print(average_prediction)\n",
    "\n",
    "denormalized_prediction = scaler_y.inverse_transform(average_prediction.reshape(-1, 1))\n",
    "\n",
    "print(denormalized_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4b979-a541-461d-b3db-d368dec661bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
